{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596127546573",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/Users/ajaffe/Dropbox/research/football/code/sfm\n"
    }
   ],
   "source": [
    "%cd ~/Dropbox/research/football/code/sfm/\n",
    "import trainsfm\n",
    "import sfmnet\n",
    "import pair_frames_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n0 1\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 6, 24, 40])"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "data_dir = '../../datasets/r3s5_tiny'\n",
    "ds=pair_frames_dataset.PairConsecutiveFramesDataset(data_dir)\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True)\n",
    "ds[0:5].shape\n",
    "# fig, (ax1, ax2) = plt.subplots(2)\n",
    "# ax1.imshow(d[1,0:3].permute(1,2,0))\n",
    "# ax2.imshow(d[1,3:6].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n0\n1\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(tensor([[[[1., 1., 1., 1.],\n           [1., 1., 1., 1.]],\n \n          [[1., 1., 1., 1.],\n           [1., 1., 1., 1.]],\n \n          [[1., 1., 1., 1.],\n           [1., 1., 1., 1.]]]], grad_fn=<GridSampler2DBackward>),\n tensor([[[[0.3413, 0.8067, 0.1366, 0.4652],\n           [0.3544, 0.6412, 0.7660, 0.5114]]]], grad_fn=<SigmoidBackward>),\n tensor([[[[0., 0.],\n           [0., 0.],\n           [0., 0.],\n           [0., 0.]],\n \n          [[0., 0.],\n           [0., 0.],\n           [0., 0.],\n           [0., 0.]]]], grad_fn=<SumBackward1>),\n tensor([[[0., 0.]]], grad_fn=<ViewBackward>))"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "\n",
    "class SfMNet(torch.nn.Module):\n",
    "  \"\"\" SfMNet is a motion detected based off a paper\n",
    "\n",
    "  The 6 input channels come from two 3 channel images concatenated\n",
    "  along the 3rd dimension \n",
    "\n",
    "  H and W must be divisible by 2**conv_depth\n",
    "  \"\"\"\n",
    "  def __init__(self, *, H, W, K=1, C=16, conv_depth=2, fc_layer_width=512):\n",
    "    super(SfMNet, self).__init__()\n",
    "    self.factor = conv_depth\n",
    "    self.H, self.W, self.K, self.C = H,W,K,C\n",
    "    self.fc_layer_width = fc_layer_width\n",
    "    # 2d affine transform\n",
    "    self.register_buffer('identity_affine_transform', \\\n",
    "      torch.tensor([[1,0,0],[0,1,0]], dtype=torch.float32))\n",
    "\n",
    "    ####################\n",
    "    #     Encoder      #\n",
    "    ####################\n",
    "    conv_encode = nn.ModuleList([nn.Conv2d(6, self.C, kernel_size=3, stride=1, padding=1, bias=False)])\n",
    "    bns_encode = nn.ModuleList([nn.BatchNorm2d(self.C)])\n",
    "    # Out channels is at most 2 ** (factor + 5) == 256 for factor == 3\n",
    "    for i in range(self.factor):\n",
    "      in_channels = self.C * (2 ** i)\n",
    "      out_channels = self.C * (2 ** (i + 1))\n",
    "      conv_encode.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False))\n",
    "      bns_encode.append(nn.BatchNorm2d(out_channels))\n",
    "      conv_encode.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "      bns_encode.append(nn.BatchNorm2d(out_channels))\n",
    "    self.conv_encode = conv_encode\n",
    "    self.bns_encode = bns_encode\n",
    "\n",
    "    ####################\n",
    "    #     Decoder      #\n",
    "    ####################\n",
    "    conv_decode = nn.ModuleList([]) \n",
    "    bns_decode = nn.ModuleList([])\n",
    "    for i in range(self.factor):\n",
    "      in_channels = int(self.C * 2 ** (self.factor - i - 1) * 1.5)\n",
    "      out_channels = self.C * 2 ** (self.factor - i - 1)\n",
    "      conv_decode.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False))\n",
    "      bns_decode.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    self.conv_decode = conv_decode\n",
    "    self.bns_decode = bns_decode\n",
    "    self.final_conv = nn.Conv2d(self.C, K, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.final_bn = nn.BatchNorm2d(K)\n",
    "\n",
    "\n",
    "    #####################\n",
    "    #     FC Layers     #\n",
    "    #####################\n",
    "    embedding_dim = self.C * H * W // (2 ** self.factor)\n",
    "    self.fc1 = nn.Linear(embedding_dim, self.fc_layer_width, bias=False)\n",
    "    self.fc2 = nn.Linear(self.fc_layer_width, self.fc_layer_width, bias=False)\n",
    "    self.fc3 = nn.Linear(self.fc_layer_width, 2*self.K, bias=False) # Predict 2d displacement for spatial transform\n",
    "\n",
    "  def get_params(self):\n",
    "    \"\"\" Get a dictionary describing the configuration of the model \"\"\"\n",
    "    return {\n",
    "      'H': self.H,\n",
    "      'W': self.W,\n",
    "      'K': self.K,\n",
    "      'C': self.C,\n",
    "      'conv_depth': self.conv_depth,\n",
    "      'fc_layer_width': self.fc_layer_width,\n",
    "    }\n",
    "\n",
    "  def total_params(self):\n",
    "    sum(p.numel() for p in self.parameters())\n",
    "\n",
    "  def forward(self, input):\n",
    "    xs = input\n",
    "    batch_size = input.shape[0]\n",
    "    # Compute the embedding using the encoder convolutional layers\n",
    "    encodings = []\n",
    "    for i, (conv, bn) in enumerate(zip(self.conv_encode, self.bns_encode)):\n",
    "      if i % 2 == 1:\n",
    "        encodings.append(xs)\n",
    "      xs = F.relu(bn(conv(xs)))\n",
    "      \n",
    "    embedding = torch.flatten(xs, start_dim=1)\n",
    "    assert(len(encodings) == self.factor)\n",
    "\n",
    "    # Compute object masks using convolutional decoder layers\n",
    "    for i, (conv, bn) in enumerate(zip(self.conv_decode, self.bns_decode)):\n",
    "      xs = F.pixel_shuffle(xs, 2)\n",
    "      xs = torch.cat((xs, encodings[-1-i]), dim=1) # Cat on channel dimension\n",
    "      xs = F.relu(bn(conv(xs)))\n",
    "\n",
    "    masks = torch.sigmoid(self.final_bn(self.final_conv(xs)))\n",
    "\n",
    "    # Compute the displacements starting from the embedding using FC layers\n",
    "    embedding = F.relu(self.fc1(embedding))\n",
    "    embedding = F.relu(self.fc2(embedding))\n",
    "    displacements = self.fc3(embedding).reshape((batch_size, self.K, 2))\n",
    "\n",
    "    # Reshape displacements and masks so they can be broadcast\n",
    "    flow = torch.sum(displacements.unsqueeze(-2).unsqueeze(-2) * masks.unsqueeze(-1), dim=1)\n",
    "    # flow has size (batch_size, H, W, 2)\n",
    "\n",
    "    # identity is not a function of any of the forward parameters\n",
    "    identity = F.affine_grid( \\\n",
    "      # Need to batchify identitiy_affine_transform\n",
    "      self.identity_affine_transform.unsqueeze(0).repeat(batch_size, 1, 1), \\\n",
    "      (batch_size, 3, self.H, self.W), \\\n",
    "      align_corners=False\n",
    "    )\n",
    "  \n",
    "    grid = identity + flow\n",
    "    out = F.grid_sample(input[:,0:3], grid, align_corners=False)\n",
    "    \n",
    "    return out, masks, flow, displacements\n",
    "\n",
    "model = SfMNet(H=2, W=4, K=1, conv_depth=1, fc_layer_width=2)\n",
    "input = torch.ones((1,6,2,4))\n",
    "model(input)"
   ]
  }
 ]
}